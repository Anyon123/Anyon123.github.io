---
layout: article
title: 卡尔曼滤波公式推导
tags: algorithm
author: Anyon
aside:
  toc: true
sidebar:
  nav: layouts
---



#### 卡尔曼滤波的迭代公式

卡尔曼滤波(kalman filter)广泛应用于实时性要求较高的状态感知领域。卡尔曼算法是一个递推算法，每次递推包括两部分内容，分别是基于已知模型的预测过程和基于测量数据的更新过程。

基于已知模型的预测过程分为两个步骤：

> 步骤一：根据$$k-1$$时刻的最优估计值$$\hat{\mathbf{x}}_{k-1}$$，通过已知模型预测下一个时刻k的估计值$$\hat{\mathbf{x}}_k$$
>
> 步骤二：根据$$k-1$$时刻的最优估计的协方差矩阵$$\mathbf{P}_{k-1}$$，通过已知模型预测下一个时刻k的协方差矩阵$$\mathbf{P}_k$$

基于测量数据的更新过程分为三个步骤：

>步骤三：根据模型预测得到的协方差矩阵$$\mathbf{P}_k$$和测量值的协方差矩阵$$\mathbf{R}_k$$得到此次递推的卡尔曼增益$$\mathbf{K}_k$$
>
>步骤四：根据卡尔曼增益$$\mathbf{K}_k$$和模型预测得到的估计值$$\hat{\mathbf{x}}_k$$以及测量得到的估计值$$\vec{\mathbf{z}}_k$$更新得到本次最优估计$$\hat{\mathbf{x}}_k^{\prime}$$
>
>步骤五：根据卡尔曼增益$$\mathbf{K}_k$$更新本次最优估计的协方差矩阵$$\mathbf{P}_k^{\prime}$$

以上步骤可用公式表达为：


$$
\begin{cases}
\hat{\mathbf{x}}_k&=\mathbf{F}_k\hat{\mathbf{x}}_{k-1}+\mathbf{B}_k\vec{\mathbf{u}}_k\\
\mathbf{P}_k&=\mathbf{F}_k\mathbf{P}_{k-1}\mathbf{F}_k^T+\mathbf{Q}_k\\
\mathbf{K}_k&=\mathbf{P}_k\mathbf{H}_k^T(\mathbf{H}_k\mathbf{P}_k\mathbf{H}_k^T-\mathbf{R}_k)^{-1}\\
\hat{\mathbf{x}}_k^{\prime}&=\hat{\mathbf{x}}_k+\mathbf{K}_k(\vec{\mathbf{z}}_k-\mathbf{H}_k\hat{\mathbf{x}}_k)\\
\mathbf{P}_k^{\prime}&=\mathbf{P}_k-\mathbf{K}_k\mathbf{H}_k\mathbf{P}_k
\end{cases}
$$


其中，$$\hat{\mathbf{x}}_{k-1}$$、$$\mathbf{P}_{k-1}$$为$$k-1$$时刻的最优估计和协方差矩阵，$$\mathbf{F}_k$$为已知模型的递推矩阵，$$\mathbf{B}_k$$为有外部控制时的控制矩阵，$$\vec{\mathbf{u}}_k$$为有外部控制时的控制量，$$\mathbf{Q}_k$$为$$\mathbf{P}_k$$在迭代过程中产生的方差矩阵，$$\vec{\mathbf{z}}_k$$为测量值向量，$$\mathbf{R}_k$$为测量值的协方差矩阵，$$\mathbf{H}_k$$为估计量转化为和测量量相同量纲和单位时的转化矩阵，$$\mathbf{K}_k$$为卡尔曼增益， $$\hat{\mathbf{x}}_k^{\prime}$$和$$\mathbf{P}_k^{\prime}$$就是本次迭代后的最优估计和协方差矩阵。

下面举个例子对上面的公式一一进行解释和说明。

#### 卡尔曼滤波公式的推导

对一个机器人的运动状态进行估计。机器人有两个状态量（位置$$p$$和速度$$v$$），这两个量都服从高斯分布，即每个量都有一个均值$$\mu$$和方差$$\sigma^2$$，均值表示最可能的状态，方差表示不确定性。位置和速度两个状态可以是没有关系的，也可以是有关系的，描述多个状态之间关系的方法是用协方差矩阵。

即需要进行估计的状态为：


$$
\hat{\mathbf{x}}_k=\left[\begin{matrix}p_k\\v_k\end{matrix}\right]
$$


状态之间的协方差矩阵为


$$
\mathbf{P}_k=\left[\begin{matrix}
\sum_{pp}&\sum_{pv}\\
\sum_{vp}&\sum_{vv}
\end{matrix}\right]
$$


根据经验对机器人的运动状态建模，我们假设机器人是匀速运动的，即每时刻速度保持不变，位置为上一时刻的位置加上速度乘以时间差，用公式表示为：


$$
\begin{cases}
p_k&=p_{k-1}+\Delta tv_{k-1}\\
v_k&=v_{k-1}
\end{cases}
$$


公式给出了基于模型的状态$\hat{x_k}$预测方法，写成矩阵的形式就是


$$
\begin{equation}\begin{aligned}
\hat{\mathbf{x}}_k&=\mathbf{F}_k\hat{\mathbf{x}}_{k-1}\\
&=\left[\begin{matrix}1&\Delta t\\0&1\end{matrix}\right]\hat{\mathbf{x}}_{k-1}
\end{aligned}\end{equation}
$$


对于分布中的每个点乘以一个矩阵$\mathbf{F}_k$后，协方差矩阵的更新公式为：


$$
\mathbf{P}_k=\mathbf{F}_k\mathbf{P}_{k-1}\mathbf{F}_k^T
$$


在具有外部控制量的情况下，比如机器人内部的动力让其加速或减速时，也会对位置和速度的状态产生影响，因此对于状态的估计建模中，还需要考虑外部控制量的影响，即


$$
\begin{equation}\begin{aligned}
p_k&=p_{k-1}+\Delta tv_{k-1}+\frac{1}{2}a\Delta t^2\\
v_k&=v_{k-1}+a\Delta t
\end{aligned}\end{equation}
$$


写成矩阵的形式为


$$
\begin{equation}\begin{aligned}
\hat{\mathbf{x}}_k&=\mathbf{F}_k\hat{\mathbf{x}}_{k-1}+\left[\begin{matrix}\frac{\Delta t_2}{2}\\\Delta t\end{matrix}\right]a\\
&=\mathbf{F}_k\hat{\mathbf{x}}_{k-1}+\mathbf{B}_k\vec{\mathbf{u}_k}
\end{aligned}\end{equation}
$$


其中$\mathbf{B}_k$为控制矩阵，$\vec{\mathbf{u}_k}$为控制向量

考虑到有外部噪声的影响，对于协方差矩阵来说，还需要加上一个$\mathbf{Q}_k$的噪声来进行处理。

预测步骤的完整表达式如下：


$$
\begin{equation}\begin{aligned}
\hat{\mathbf{x}}_k&=\mathbf{F}_k\hat{\mathbf{x}}_{k-1}+\mathbf{B}_k\vec{\mathbf{u}_k}\\
\mathbf{P}_k&=\mathbf{F}_k\mathbf{P}_{k-1}\mathbf{F}_k^T+\mathbf{Q}_k
\end{aligned}\end{equation}
$$


上面的过程是基于当前时刻的状态，用经验模型预测出来的下一时刻的状态，卡尔曼滤波同时会考虑实际测量值对这个经验模型预测出来的值进行修正。

测量值是由传感器得到的，测量值也是有一定误差的，并且满足高斯分布，我们令测量值的分布的均值为$\vec{\mathbf{z}}_k$，其协方差为$\mathbf{R}_k$，即测量值满足：$\mathcal N_2\sim(x,\vec{\mathbf{z}}_k, \mathbf{R}_k)$

因为传感器读取数据的单位和尺度有可能和我们要跟踪的量不一样，比如：我们要跟踪的量$$\hat{\mathbf{x}}_k$$包含位置$$p$$和速度$$v$$两个量，而传感器可能只有测量速度的传感器，位置$$p$$无法通过传感器获取测量值，因此需要引入一个测量转换矩阵$$\mathbf{H}_k$$来保证需要跟踪的量和测量得到的数据维度单位是一致的。注意这里是将测量矩阵$$\mathbf{H}_k$$左乘以需要预测得到的量$$\hat{\mathbf{x}}_k$$，转化为与测量值相同的量纲。当然协方差也需要做相应的转换：


$$
\begin{equation}\begin{aligned}
\vec{\mu}_{excepted}&=\mathbf{H}_k\hat{\mathbf{x}}_k\\
\textstyle\sum_{excepted}&=\mathbf{H}_k\mathbf{P}_k\mathbf{H}_k^T
\end{aligned}\end{equation}
$$


于是，我们基于模型预测得到的量可以和测量值量纲相同的高斯分布$$\mathcal N_1\sim(x,\vec{\mu}_{excepted},\textstyle\sum_{excepted})$$。

好的，那么现在我们得到了两个高斯分布，两个高斯分布中重叠的部分的均值也就是两个估计中最可能的值，也是所有给定信息中的最优估计。我们现在需要把两个高斯分布相乘得到一个新的高斯分布，来更新我们的状态估计值。两个高斯分布相乘的方法如附件所述。这里直接给出相乘之后的均值和方差。

对于两个单变量的高斯分布，相乘之后有：


$$
\begin{equation}\begin{aligned}
\mu^{\prime}&=\frac{\sigma_1^2\mu_2+\sigma_2^2\mu_1}{\sigma_1^2+\sigma_2^2}\\
\sigma^{\prime2}&=\frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}
\end{aligned}\end{equation}
$$


令


$$
k=\frac{\sigma_1^2}{\sigma_1^2+\sigma_2^2}
$$


则可将公式转化为：


$$
\begin{equation}\begin{aligned}
\mu^{\prime}&=\mu_1+k(\mu_2-\mu_1)\\
\sigma^{\prime2}&=\sigma_1^2-k\sigma_1^2
\end{aligned}\end{equation}
$$


上面是单变量的高斯分布相乘，改成矩阵形式如下：


$$
\begin{equation}\begin{aligned}
\mathbf{K}&=\textstyle\sum_1(\sum_1+\sum_2)^{-1}\\
\vec{\mu}^{\prime}&=\vec{\mu}_1+\mathbf{K}(\vec{\mu}_2-\vec{\mu}_1)\\
\textstyle\sum^{\prime}&=\textstyle\sum_1-\mathbf{K}\sum_1
\end{aligned}\end{equation}
$$


将我们推导的两个高斯分布（$$\mathcal N_1\sim(x,\vec{\mu}_{excepted},\textstyle\sum_{excepted})$$，$$\mathcal N_2\sim(x,\vec{\mathbf{z}}_k, \mathbf{R}_k)$$）带入到公式$$$$中，得到：


$$
\begin{equation}\begin{aligned}
\mathbf{K}&=\mathbf{H}_k\mathbf{P}_k\mathbf{H}_k^T(\mathbf{H}_k\mathbf{P}_k\mathbf{H}_k^T+\mathbf{R}_k)^{-1}\\
\mathbf{H}_k\hat{\mathbf{x}}_k^{\prime}&=\mathbf{H}_k\hat{\mathbf{x}}_k+\mathbf{K}(\vec{\mathbf{z}}_k-\mathbf{H}_k\hat{\mathbf{x}}_k)\\
\mathbf{H}_k\mathbf{P}_k^{\prime}\mathbf{H}_k^T&=\mathbf{H}_k\mathbf{P}_k\mathbf{H}_k^T-\mathbf{K}\mathbf{H}_k\mathbf{P}_k\mathbf{H}_k^T
\end{aligned}\end{equation}
$$


消去等号左侧的$\mathbf{H}_k$和$\mathbf{H}_k^T$，可得：


$$
\begin{equation}\begin{aligned}
\mathbf{K}^{\prime}&=\mathbf{P}_k\mathbf{H}_k^T(\mathbf{H}_k\mathbf{P}_k\mathbf{H}_k^T+\mathbf{R}_k)^{-1}\\
\hat{\mathbf{x}}_k^{\prime}&=\hat{\mathbf{x}}_k+\mathbf{K}^{\prime}(\vec{\mathbf{z}}_k-\mathbf{H}_k\hat{\mathbf{x}}_k)\\
\mathbf{P}_k^{\prime}&=\mathbf{P}_k-\mathbf{K}^{\prime}\mathbf{H}_k\mathbf{P}_k
\end{aligned}\end{equation}
$$


其中$\mathbf{K}^{\prime}$称为卡尔曼增益，$\hat{\mathbf{x}}_k^{\prime}$和$\mathbf{P}_k^{\prime}$就是经过测量数据矫正后得最优估计，可以在下一轮预测和更新中使用。

公式$$和公式$$就是我们最开始写出的卡尔曼滤波的递推公式。





#### 附：

两个高斯分布相乘得到一个新的压缩或拉伸过的高斯分布，推导过程如下：

高斯分布1:


$$
\mathcal N_1(x,\mu_1,\sigma_1)=\frac{1}{\sqrt{2\pi}\sigma_1}e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}}
$$


高斯分布2：


$$
\mathcal N_2(x,\mu_2,\sigma_2)=\frac{1}{\sqrt{2\pi}\sigma_2}e^{-\frac{(x-\mu_2)^2}{2\sigma_2^2}}
$$


相乘之后的高斯分布为：


$$
\begin{equation}\begin{aligned}
\mathcal N^{\prime}(x,\mu^{\prime},\sigma^{\prime}) &= \mathcal N_1(x,\mu_1,\sigma_1)\times\mathcal N_2(x,\mu_2,\sigma_2)\\
&=\frac{1}{2\pi\sigma_1\sigma_2}e^{-\frac{\sigma_2^2(x-\mu_1)^2+\sigma_1^2(x-\mu_2)^2}{2\sigma_1^2\sigma_2^2}}
\end{aligned}\end{equation}
$$


关注指数部分，令


$$
\begin{equation}\begin{aligned}
\beta&=\frac{\sigma_2^2(x-\mu_1)^2+\sigma_1^2(x-\mu_2)^2}{2\sigma_1^2\sigma_2^2}\\
&=\frac{(\sigma_1^2+\sigma_2^2)x^2-2(\sigma_1^2\mu_2+\sigma_2^2\mu_1)x+(\sigma_1^2\mu_2^2+\sigma_2^2\mu_1^2)}{2\sigma_1^2\sigma_2^2}\\
&=\frac{x^2-2\frac{\sigma_1^2\mu_2+\sigma_2^2\mu_1}{\sigma_1^2+\sigma_2^2}x+\frac{\sigma_1^2\mu_2^2+\sigma_2^2\mu_1^2}{\sigma_1^2+\sigma_2^2}}{2\frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}}\\
&=\frac{(x-\frac{\sigma_1^2\mu_2+\sigma_2^2\mu_1}{\sigma_1^2+\sigma_2^2})^2}{2\frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}}+\frac{\frac{\sigma_1^2\mu_2^2+\sigma_2^2\mu_1^2}{\sigma_1^2+\sigma_2^2}-(\frac{\sigma_1^2\mu_2+\sigma_2^2\mu_1}{\sigma_1^2+\sigma_2^2})^2}{2\frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}}
\end{aligned}\end{equation}
$$


设


$$
\begin{align}
\gamma&=\frac{(x-\frac{\sigma_1^2\mu_2+\sigma_2^2\mu_1}{\sigma_1^2+\sigma_2^2})^2}{2\frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}}\\
\lambda&=\frac{\frac{\sigma_1^2\mu_2^2+\sigma_2^2\mu_1^2}{\sigma_1^2+\sigma_2^2}-(\frac{\sigma_1^2\mu_2+\sigma_2^2\mu_1}{\sigma_1^2+\sigma_2^2})^2}{2\frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}}
\end{align}
$$


则公式$$ 可以写成如下形式：


$$
\begin{equation}\begin{aligned}
\mathcal N^{\prime}&=\frac{1}{2\pi\sigma_1\sigma_2}e^{-(\gamma+\lambda)}\\
&=S\times\frac{1}{\sqrt{2\pi}\sigma^{\prime}}e^{-\frac{(x-\mu^{\prime})^2}{2\sigma^{\prime2}}}
\end{aligned}\end{equation}
$$


其中：


$$
\begin{equation}\begin{aligned}
\mu^{\prime}&=\frac{\sigma_1^2\mu_2+\sigma_2^2\mu_1}{\sigma_1^2+\sigma_2^2}\\
\sigma^{\prime2}&=\frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}\\
S&=\frac{\sigma^{\prime}}{\sqrt{2\pi}\sigma_1\sigma_2}e^{-\lambda}
\end{aligned}\end{equation}
$$


把$\lambda$进行化简


$$
\begin{equation}\begin{aligned}
\lambda&=\frac{(\sigma_1^2\mu_2^2+\sigma_2^2\mu_1^2)(\sigma_1^2+\sigma_2^2)-(\sigma_1^2\mu_2+\sigma_2^2\mu_1)^2}{2\sigma_1^2\sigma_2^2(\sigma_1^2+\sigma_2^2)}\\
&=\frac{\sigma_1^4\mu_2^2+(\mu_1^2+\mu_2^2)\sigma_1^2\sigma_2^2+\sigma_2^4\mu_1^2-\sigma_1^4\mu_2^2-2\sigma_1^2\sigma_2^2\mu_1\mu_2-\sigma_2^4\mu_1^2}{2\sigma_1^2\sigma_2^2(\sigma_1^2+\sigma_2^2)}\\
&=\frac{(\mu_1-\mu_2)^2}{2(\sigma_1^2+\sigma_2^2)}
\end{aligned}\end{equation}
$$


将$\lambda$代入到公式$$的$S$表达式中，可得


$$
\begin{equation}\begin{aligned}
S=\frac{1}{\sqrt{2\pi(\sigma_1^2+\sigma_2^2)}}e^{-\frac{(\mu_1-\mu_2)^2}{2(\sigma_1^2+\sigma_2^2)}}
\end{aligned}\end{equation}
$$


$S$为一个常数，所以两个高斯分布相乘以后仍然是一个高斯分布，当$S<1$时，新的高斯分布被压缩了，当$S>1$时，新的高斯分布被拉伸了。其实这里主要是为了推导得到新的高斯分布的均值和方差。

